{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21006a0e",
   "metadata": {},
   "source": [
    "# Behind the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c834c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\envs\\ai\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97136316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifer (\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")\n",
    "classifer = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "distilbert\n",
    "\n",
    "BERT-family model\n",
    "\n",
    "Encoder-only\n",
    "\n",
    "base\n",
    "\n",
    "Hidden size ~768\n",
    "\n",
    "uncased\n",
    "\n",
    "Lowercases input\n",
    "\n",
    "Vocabulary does not preserve capitalization\n",
    "\n",
    "finetuned-sst-2\n",
    "\n",
    "Classification head trained on sentiment\n",
    "\n",
    "english\n",
    "\n",
    "Language assumptions baked into tokenizer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365acd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding = True, truncation = True, return_tensors= \"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a715f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be377e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\") #defiens the architecture directly instead of using AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fb402",
   "metadata": {},
   "source": [
    "## Loading and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\") # save the model's weights and architecture configuration\n",
    "# save 2 files config.json: contain all the necessary attributes needed to build the model architecture, also contain the metadata\n",
    "#ytorch_model.safetensors: state dictionary, contains all the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use saved model:\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"directory_on_my_computer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ab4d1",
   "metadata": {},
   "source": [
    "## logging to huggin face from a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297797b4",
   "metadata": {},
   "source": [
    "## push model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"my-awesome-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a783796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"your-username/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3096ea5",
   "metadata": {},
   "source": [
    "## Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, this is a single sentence!\")\n",
    "print(encoded_input)\n",
    "\"\"\"\n",
    "the output dictionary:\n",
    "input_ids: numerical representaion of the tokens\n",
    "token_type_ids: tells the model which part of the input is sentence A and which is sentence BaseException\n",
    "attention_musk: indicates which tokens should be attended to and whihc should not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can decode the input IDs to get back the original text:\n",
    "tokenizer.decode(encoded_input[\"input_ids\"]) \n",
    "#[CLS] and [SEP] are special tokens add by the tokenizer as they aiter required by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa83db",
   "metadata": {},
   "source": [
    "### encode multiple sentences at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ab7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"])\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer (\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], return_tensors=\"np\")#return numpy array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2434721",
   "metadata": {},
   "source": [
    "### Padding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ac40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two lists dont have the same length and the arrays and tensors need to be rectangular\n",
    "# we solve this problem by pad the inputs that will make all sentences the same length by adding special tokenizer\n",
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], return_tensors=\"pt\", padding=True)#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739109a",
   "metadata": {},
   "source": [
    "### truncating inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation parameter used to make BERT model able to process more tokens than 512 token \n",
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining padding and trancation arguments can make the tensors have the exact needed size\n",
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], \n",
    "                           return_tensors=\"pt\", \n",
    "                           padding=True,\n",
    "                           max_length = 5,\n",
    "                           )#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d114b",
   "metadata": {},
   "source": [
    "### adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4701293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens are added in BERT to better represent the sentnece boundaries\n",
    "# such as the beginnig of the sentece ([CLS]) or separator between sentences ([SEP])\n",
    "encoded_input = tokenizer (\"How are you\")\n",
    "print (encoded_input[\"input_ids\"])\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])\n",
    "# not al models need special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf0502",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520489d5",
   "metadata": {},
   "source": [
    "## from token to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abcfac",
   "metadata": {},
   "source": [
    "## decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7849cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode ([7993, 170, 13809, 23763, 2443, 1110, 3014])\n",
    "print (decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e195c1a",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccccf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])  \n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)#the model expect multiple sentences by default\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ee673",
   "metadata": {},
   "source": [
    "# Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "model_inputs = tokenizer(sequence) # model_input contain everything necessary for a modle to operate well\n",
    "                                  # distilbert needs input IDs and attention mask\n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3706b1e",
   "metadata": {},
   "source": [
    "## handling multiple sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a16565",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f404df",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding = \"longest\")\n",
    "\n",
    "# pad the sequences up to the model max lenth (512 fro BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "#pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67848f53",
   "metadata": {},
   "source": [
    "## truncating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# truncate the sequences that are longer than the model max length\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "#trancate the sequences that are longer than the specified max lenght\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb6843",
   "metadata": {},
   "source": [
    "## return tensors from different frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding = True, return_tensors=\"pt\")\n",
    "print(model_inputs)\n",
    "\n",
    "# returns NumPy arrays \n",
    "model_inputs = tokenizer(sequences, padding = True, return_tensors=\"np\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a1679",
   "metadata": {},
   "source": [
    "## special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb6d58",
   "metadata": {},
   "source": [
    "### single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "input_ids = model_inputs[\"input_ids\"]\n",
    "print(input_ids)\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded611a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(input_ids))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ad730",
   "metadata": {},
   "source": [
    "### multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sequences)\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423db092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences, padding = True, return_tensors=\"pt\",truncation=True)\n",
    "input_ids = model_inputs[\"input_ids\"]\n",
    "print(input_ids)\n",
    "\n",
    "\n",
    "# input_ids is a 2D tensor (batch_size Ã— sequence_length).\n",
    "# tokenizer.decode() expects a single 1D sequence of token IDs.\n",
    "# tokenizer.batch_decode() is used to decode multiple sequences (a batch).\n",
    "print(tokenizer.batch_decode(input_ids)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169c52c",
   "metadata": {},
   "source": [
    "## Wrapping up: from tokenizer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e384f6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # Loads the tokenizer (vocabulary, tokenization rules, and special tokens) that exactly matches the model checkpoint, ensuring text is converted into the correct token IDs expected by the model's embedding layer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint) # Loads a pretrained transformer model along with a task-specific sequence classification head, restoring all learned weights so the model can map token sequences to classification logits \n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] #fuly spcified model package hosted by hugging face\n",
    "\n",
    "tokens = tokenizer(sequences, padding = True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)# The **tokens syntax unpacks the dictionary so the model receives the arguments it expects.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff434ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf0221",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ai (Python 3.13.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
