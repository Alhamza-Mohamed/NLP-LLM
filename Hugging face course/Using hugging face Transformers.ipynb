{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21006a0e",
   "metadata": {},
   "source": [
    "# Behind the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97136316",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = pipeline(\"sentiment-analysis\")\n",
    "classifer (\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "distilbert\n",
    "\n",
    "BERT-family model\n",
    "\n",
    "Encoder-only\n",
    "\n",
    "base\n",
    "\n",
    "Hidden size ~768\n",
    "\n",
    "uncased\n",
    "\n",
    "Lowercases input\n",
    "\n",
    "Vocabulary does not preserve capitalization\n",
    "\n",
    "finetuned-sst-2\n",
    "\n",
    "Classification head trained on sentiment\n",
    "\n",
    "english\n",
    "\n",
    "Language assumptions baked into tokenizer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365acd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding = True, truncation = True, return_tensors= \"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a715f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be377e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\") #defiens the architecture directly instead of using AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fb402",
   "metadata": {},
   "source": [
    "## Loading and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\") # save the model's weights and architecture configuration\n",
    "# save 2 files config.json: contain all the necessary attributes needed to build the model architecture, also contain the metadata\n",
    "#ytorch_model.safetensors: state dictionary, contains all the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use saved model:\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"directory_on_my_computer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ab4d1",
   "metadata": {},
   "source": [
    "## logging to huggin face from a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297797b4",
   "metadata": {},
   "source": [
    "## push model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"my-awesome-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a783796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"your-username/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3096ea5",
   "metadata": {},
   "source": [
    "## Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 1142, 1110, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, this is a single sentence!\")\n",
    "print(encoded_input)\n",
    "\"\"\"\n",
    "the output dictionary:\n",
    "input_ids: numerical representaion of the tokens\n",
    "token_type_ids: tells the model which part of the input is sentence A and which is sentence BaseException\n",
    "attention_musk: indicates which tokens should be attended to and whihc should not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6dd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hello, this is a single sentence! [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can decode the input IDs to get back the original text:\n",
    "tokenizer.decode(encoded_input[\"input_ids\"]) \n",
    "#[CLS] and [SEP] are special tokens add by the tokenizer as they aiter required by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa83db",
   "metadata": {},
   "source": [
    "### encode multiple sentences at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217ab7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1731, 1132, 1128, 136, 102], [101, 146, 112, 182, 2503, 117, 6243, 1128, 106, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"])\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c311baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,  146,  112,  182, 2503,  117, 6243,\n",
      "         1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer (\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab86bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([array([ 101, 1731, 1132, 1128,  136,  102]),\n",
      "       array([ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102])],\n",
      "      dtype=object), 'token_type_ids': array([array([0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
      "      dtype=object), 'attention_mask': array([array([1, 1, 1, 1, 1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
      "      dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], return_tensors=\"np\")#return numpy array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2434721",
   "metadata": {},
   "source": [
    "### Padding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b48ac40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# the two lists dont have the same length and the arrays and tensors need to be rectangular\n",
    "# we solve this problem by pad the inputs that will make all sentences the same length by adding special tokenizer\n",
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], return_tensors=\"pt\", padding=True)#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739109a",
   "metadata": {},
   "source": [
    "### truncating inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994c7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# truncation parameter used to make BERT model able to process more tokens than 512 token \n",
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2420bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\ai\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2914: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# combining padding and trancation arguments can make the tensors have the exact needed size\n",
    "encoded_input = tokenizer ([\"How are you?\", \"I'm fine, thank you!\"], \n",
    "                           return_tensors=\"pt\", \n",
    "                           padding=True,\n",
    "                           max_length = 5,\n",
    "                           )#return pytorch array\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d114b",
   "metadata": {},
   "source": [
    "### adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4701293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1731, 1132, 1128, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] How are you [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens are added in BERT to better represent the sentnece boundaries\n",
    "# such as the beginnig of the sentece ([CLS]) or separator between sentences ([SEP])\n",
    "encoded_input = tokenizer (\"How are you\")\n",
    "print (encoded_input[\"input_ids\"])\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])\n",
    "# not al models need special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d59d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
